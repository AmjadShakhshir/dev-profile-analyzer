# ğŸ“Š dev-profile-analyzer

A high-performance Python program that processes JSON profiles, extracts company and skill data, and computes conditional probabilities.

## ğŸš€ Features

- âœ… **Processes JSON profiles** in bulk using multiprocessing
- âœ… **Extracts unique companies and skills** from JSON files
- âœ… **Counts occurrences** of companies and skills
- âœ… **Filters profiles that have fewer than 3 companies** (only profiles with **3+ companies** are considered)
- âœ… **Computes conditional probabilities** `P(Skill | Company)`
- âœ… **Uses orjson for fast JSON parsing**
- âœ… **Utilizes defaultdict for optimized dictionary operations**
- âœ… **Parallel processing with workers (multiprocessing)** to speed up batch processing

---

## ğŸ“‚ Project Structure
```pgsql
â”œâ”€â”€ README.md
â”œâ”€â”€ __pycache__
â”œâ”€â”€ data_processing.py
â”œâ”€â”€ file_utils.py
â”œâ”€â”€ main.py
â”œâ”€â”€ profiles
â””â”€â”€ requirements.txt
```

---

## âš¡ Installation

1. **Clone the repository**
```sh
   git clone https://github.com/your-username/dev-profile-analyzer.git
   cd dev-profile-analyzer
```

2. **Create a virtual environment**
```sh
    python -m venv venv
    source venv/bin/activate  # On macOS/Linux
    venv\Scripts\activate     # On Windows
```

3. **Install dependencies**
```sh
    pip install -r requirements.txt
```

4. **Profiles folder**
```sh
    https://drive.google.com/file/d/1En6JSPE0o3vMxf6l9W8fn4RQAwIbSZx7/view
```

## ğŸ“œ Usage
**To run the program, execute:**
```sh
    python src/main.py
```

## **This will:**
    - Load all JSON profiles from the profiles/ directory.
    - Process them in parallel using multiprocessing workers.
    - Filter out profiles that have fewer than 3 companies.
    - Compute conditional probabilities P(Skill | Company).
    - Print the results in the following format:
```less
    P(Go | Amazon): 0.4722445695897024
    P(Rust | Amazon): 0.4648028962188254
```

## ğŸ›  Workflow Diagram

![flowchart](assets/Dev-profiles-flowchart.jpg)


## ğŸ—ï¸ High-Level Design

![design](assets/high-level-abstraction.jpg)

## ğŸ” How It Works
    1ï¸âƒ£ Read JSON Files
        * Uses orjson.loads() for fast and efficient JSON parsing.

    2ï¸âƒ£ Extract Companies and Skills
        * Extracts unique values from companies and skills fields.
    
    3ï¸âƒ£ Filter Profiles with Less Than 3 Companies
        * Profiles with fewer than 3 companies are skipped to ensure meaningful probability calculations.

    4ï¸âƒ£ Count Occurrences
        * Uses defaultdict(int) to efficiently count occurrences.

    5ï¸âƒ£ Compute Conditional Probabilities
        * Uses joint probability and Bayes' theorem to calculate P(Skill | Company).

    6ï¸âƒ£ Parallel Processing with Workers
        * The program uses multiprocessing workers to speed up batch processing.
        * Number of workers is set in NUM_WORKERS = 4 (configurable).
        * Each worker processes a batch of JSON files in parallel, reducing execution time.


## ğŸ“Œ Example JSON Profile
```json
    {
        "name": "John Doe",
        "companies": ["Amazon"],
        "skills": ["Go", "Rust"]
    }
```

## ğŸ›  Dependencies
    - Python 3.8+
    - orjson (Fast JSON processing)
    - multiprocessing (Efficient batch processing)

## Install dependencies via:
```sh
    pip install -r requirements.txt
```

## ğŸ”´ Core Bottlenecks (Why Are These Challenges?)
    1ï¸âƒ£ Memory Bottleneck â€“ Large Dataset (10,000 Files)
        â“ Why is this a problem?
            ğŸ’¡ Because loading everything at once could cause memory overflow and slow down execution.
        â“ Why do we need a better approach?
            ğŸ’¡ Because an inefficient method would waste computation time and make debugging difficult.
    âœ… Solution: Use an efficient file loading strategy (Streaming/Batches/Parallel Execution).

    2ï¸âƒ£ I/O Bottleneck â€“ File Processing Speed
        â“ Why is this a problem?
            ğŸ’¡ Reading 10,000 individual files one-by-one can be very slow.
        â“ Why do we need a faster approach?
            ğŸ’¡ Because the bigger the dataset, the longer it takes to extract meaningful insights.
    âœ… Solution: Batch processing or parallel execution could improve speed.


